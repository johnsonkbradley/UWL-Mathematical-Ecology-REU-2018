---
title: "R Notebook"
output: html_notebook
---

Libraries
===

```{r}
library(dplyr)
library(ggplot2)
library(ggmap)
library(car)
library(GGally)
library(lubridate)
library(cluster)
library(factoextra)
##pulling from api packages
library(rtweet)
library(httr)
library(jsonlite)
```

Load in the keys
===

```{r}
##flickr keys
keys <- read.table("flickrAPIkeys.txt", sep = ",", header = T) #have key and secret
##twitter keys
twitterKeys <- read.table("twitterAPIkeys.txt", sep = ",", header = T)
twitterKeys$consumer_key <- as.character(twitterKeys$consumer_key)
twitterKeys$consumer_secret <- as.character(twitterKeys$consumer_secret)
twitterKeys$access_token <- as.character(twitterKeys$access_token)
twitterKeys$access_secret <- as.character(twitterKeys$access_secret)
```

Flickr functions
===

```{r}
## This function will give you all of the flickr photos for a given query, starting from a certain date that the photos were taken
## Inputs: the search query you want, the earliest day you want to grab that a photo was taken on
## OUtputs: all flickr posts matching the qeury and the minimum photo taken date that you specified
GetPhotos <- function(query, startDate) {
  query <- gsub(" ", "+", query)
  startDate <- gsub("/", "%2F", startDate)
  url <- paste("https://api.flickr.com/services/rest/?method=flickr.photos.search&api_key=",keys$key,"&text=",query,"&min_taken_date=",startDate,"&sort=date-taken-asc&content_type=1&per_page=5000&format=json&nojsoncallback=1&api_sig=",keys$secret, sep = "")
  photos <- GET(url = url)%>%content(as = "text")%>%fromJSON()
  photosDF <- photos$'photos'$photo
  return(photosDF)
}

## This function will return the date that a flickr photo was taken, given the id number of the picture
## Inputs: the id number of the picture you want to know the date taken
## Outputs: the date taken
GetDate <- function(id) {
  url <- paste("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=",keys$key,"&photo_id=",id,"&format=json&nojsoncallback=1&api_sig=",keys$secret, sep = "")
  info <- GET(url = url)%>%content(as = "text")%>%fromJSON()
  date <- info$`photo`$dates$taken
  return(date)
}
```

Setting up the Twitter Token (so we can use the API)
===

```{r}
create_token(app = "REU 2018", consumer_key = twitterKeys$consumer_key, consumer_secret =  twitterKeys$consumer_secret, access_token = twitterKeys$access_token, access_secret = twitterKeys$access_secret)
```

How to search using rtweets (test)
===

```{r}
mendota1 <- search_tweets(q = "Lake Mendota")
ts_plot(mendota1) ##we can see the number of tweets per time; ggplot style; 9 day cap for pulls
mendota1 ##this is saved as a list, so we cant save in a csv file to send save as an rda file


waubesa1 <- search_tweets(q = "Lake Waubesa")
ts_plot(waubesa1)
waubesa1

##saving as an rda file
save(mendota1, file = "mendota6.20.2018.rda")
```



Pull Data From flickr for Lake Mendota (test)
===

```{r}
##starting is 1/1/2013 - end 12/31/2017
mendotaFlickr5 <- GetPhotos(query = "Lake Mendota", startDate = "5/7/2018")
dim(mendotaFlickr5) ##gets 500 flickr photos
mendotaFlickr5$DateTaken <- sapply(mendotaFlickr5$id, GetDate) ##this takes awhile
mendotaFlickr5 ##got 500 photos from the start date


mendotaFlickr1.1.2013_12.5.2013 
mendotaFlickr12.6.2013_8.16.2014 
mendotaFlickr8.17.2014_6.12.2015
mendotaFlickr6.13.2015_1.5.2016 
mendotaFlickr1.6.2016_3.14.2016 
mendotaFlickr3.15.2016_10.23.2016
mendotaFlickr10.24.2016_5.6.2018  ## can stop here if we only care about 2013-2017
mendotaFlickr5.7.2018_6.16.2018 ##with this we have all the posts from lake mendota from 1/1/2013 - 6/16/2018
##putting these all into one big data set
mendotaFlickr5year <- rbind(mendotaFlickr1.1.2013_12.5.2013, mendotaFlickr12.6.2013_8.16.2014, mendotaFlickr8.17.2014_6.12.2015, mendotaFlickr6.13.2015_1.5.2016, mendotaFlickr1.6.2016_3.14.2016, mendotaFlickr3.15.2016_10.23.2016, mendotaFlickr10.24.2016_5.6.2018, mendotaFlickr5.7.2018_6.16.2018)

dim(mendotaFlickr5year) ##3559 photos in a about 5.5 years
```

Will need to filter out the duplicate photos to get the actual number of users, then seperate it down by year
===

```{r}
mendotaFlickr5year$DateTaken[1]

mendotaFlickr5year$Year <- substr(mendotaFlickr5year$DateTaken, 1, 4)
mendotaFlickr5year$Month <- substr(mendotaFlickr5year$DateTaken, 6, 7)
mendotaFlickr5year$Day <- substr(mendotaFlickr5year$DateTaken, 9, 10)

mendotaFlickr5year
dim(mendotaFlickr5year) ##3559 different photos

##want to search for only 1 owner on each data
uniqueDays <- mendotaFlickr5year%>%group_by(owner, Year, Month, Day)%>%summarise(Count = n())
dim(uniqueDays) ##1201 different visitors total

visitorsPerMonth <- uniqueDays%>%group_by(Year, Month)%>%summarise(Count = n())
visitorsPerMonth$EasyDay <- paste(visitorsPerMonth$Year, visitorsPerMonth$Month, sep = "-")
visitorsPerMonth$Group = 1


visitorsPerYear <- uniqueDays%>%group_by(Year)%>%summarise(Count = n())
visitorsPerYear$Group = 1
visitorsPerYear

ggplot(data = visitorsPerMonth, aes(x = EasyDay, y = Count, group = Group)) + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Month") + ylab("Number of Visitors") + ggtitle("Flickr Visitors to Lake Mendota Per Month")
ggplot(data = visitorsPerYear, aes(x = Year, y = Count, group = Group)) + geom_line() + xlab("Year") + ylab("Number of Visitors") + ggtitle("Flickr Visitors to Lake Mendota Per Year") + scale_y_continuous(breaks=c(0, 50, 100, 150, 200, 250, 300), limits = c(0, 300)) 

##write.csv(visitorsPerMonth, "lakeMendotaVisPerMonth.csv", row.names = F)
```


