---
title: "R Notebook"
output: html_notebook
---

Libraries
===

```{r}
##general
library(dplyr)
library(ggplot2)
library(ggmap)
library(car)
library(GGally)
library(lubridate)
library(cluster)
library(factoextra)
library(rgdal)
library(sf)
library(pscl)
library(gmapsdistance)
library(forecast)
library(reshape2)
library(car)
library(lubridate)
##social media
library(rtweet)
library(httr)
library(jsonlite)
```

## Flickr

Flickr functions *make better sub-sections for this*
===

```{r}
GetPhotosBBox <- function(query, startDate, bbox) {
  query <- gsub(" ", "+", query)
  startDate <- gsub("/", "%2F", startDate)
  url <- paste("https://api.flickr.com/services/rest/?method=flickr.photos.search&api_key=",keys$key,"&text=",query,"&min_taken_date=",startDate,"&bbox=",bbox,"&sort=date-taken-asc&content_type=1&per_page=500&format=json&nojsoncallback=1&api_sig=",keys$secret, sep = "")
  photos <- GET(url = url)%>%content(as = "text")%>%fromJSON()
  photosDF <- photos$'photos'$photo
  return(photosDF)
}

## This function will return the date that a flickr photo was taken, given the id number of the picture
## Inputs: the id number of the picture you want to know the date taken
## Outputs: the date taken
GetDate <- function(id) {
  url <- paste("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=",keys$key,"&photo_id=",id,"&format=json&nojsoncallback=1&api_sig=",keys$secret, sep = "")
  info <- GET(url = url)%>%content(as = "text")%>%fromJSON()
  date <- info$`photo`$dates$taken
  return(date)
}

## This function will return the home location listed by each flickr user
## Inputs: user id
## Outputs: home location for the poster of the photo
GetHomeLocation <- function(user) {
  url <- paste("https://api.flickr.com/services/rest/?method=flickr.people.getInfo&api_key=",keys$key,"&user_id=",user,"&format=json&nojsoncallback=1&api_sig=",keys$secret, sep = "")
  info <- GET(url = url)%>%content(as = "text")%>%fromJSON()
  location <- info$`person`$location$`_content`
  return(location)
}

## This function will take the raw flickr posts data and will return a data frame containing the number of visitors per month
## Inputs: raw flickr data for the past 5 years
## Outputs: summary of the number of visitors per month
GetVPM <- function(dataFlickr5) {
  dataFlickr5$Year <- substr(dataFlickr5$DateTaken, 1, 4)
  dataFlickr5$Month <- substr(dataFlickr5$DateTaken, 6, 7)
  dataFlickr5$Day <- substr(dataFlickr5$DateTaken, 9, 10)

  dataUnique <- dataFlickr5%>%group_by(owner, Year, Month, Day)%>%summarise(Count = n())

  dataMonth <- dataUnique%>%group_by(Year, Month)%>%summarise(Count = n())
  dataMonth$Date <- paste(dataMonth$Year, dataMonth$Month, sep = "-")
  dataMonth$Group <- 1
  
  dataMonth2 <- DATES%>%left_join(dataMonth[,-c(1,2)], by = "Date")
  dataMonth2$Count[is.na(dataMonth2$Count)] <- 0
  dataMonth2$Group <- 1
  
  return(dataMonth2)
}
```

Twitter and Flickr functions for DistanceDriven, DaysSince, and LengthTrip
===

## DistanceDriven functions

```{r}
## This function will get the distances between the user specified home location, and the park that they visited
## Inputs: A twitter data frame
## Outputs: twitter data frame
TwitterDistance <- function(data) {
  data$DistanceDriven <- NA
  data$Name2 <- gsub(pattern = replacePattern, replacement = "+", x = data$Name)
  data$Location2 <- gsub(pattern = replacePattern, replacement = "+", x = data$location)
  
  for (i in 1:nrow(data)) {
    if (nchar(data$Location2[i]) > 1) {
      data$DistanceDriven[i] <- gmapsdistance(origin = data$Location2[i], destination = data$Name2[i], mode = "driving")$Distance / 1609
    }
  }
  return(data)
}

## This function will get the distances between the user specified home location, and the park that they visited
## Inputs: A flickr data frame
## Outputs: flick data frame
FlickrDistance <- function(data) {
  data$DistanceDriven <- NA
  data$Name2 <- gsub(pattern = replacePattern, replacement = "+", x = data$Name)
  data$Location2 <- gsub(pattern = replacePattern, replacement = "+", x = data$Location)
  
  for (i in 1:nrow(data)) {
    if (nchar(data$Location2[i]) > 1) {
      data$DistanceDriven[i] <- gmapsdistance(origin = data$Location2[i], destination = data$Name2[i], mode = "driving")$Distance / 1609
    }
  }
  return(data)
}
```

## DaysSince function

```{r}
## This function will compute the days since a visitor's last visit to a park
## Inputs: flickr data frame
## Outputs: flickr data frame
FlickrComputeDaysSince <- function(data) {
  data$Date <- paste(data$Day, data$Month, data$Year, sep = ".")

  data <- data%>%arrange(Name, owner, Year, Month, Day)

  ##actually computing days since
  data$DaysSince <- NA
  for (i in 2:nrow(data)) {
    if((data$owner[i] == data$owner[i-1]) & (data$Name[i] == data$Name[i-1])) {
      data$DaysSince[i] <- difftime(strptime(data$Date[i], format = "%d.%m.%Y"),
                                           strptime(data$Date[i-1], format = "%d.%m.%Y"), units = "days")
    }
  }
  return(data)
}

## This function will compute the days since a visitor's last visit to a park
## Inputs: twitter data frame
## Outputs: twitter data frame
TwitterComputeDaysSince <- function(data) {
  data$Date <- paste(data$Day, data$Month, data$Year, sep = ".")

  data <- data%>%arrange(Name, user_id, Year, Month, Day)

  ##actually computing days since
  data$DaysSince <- NA
  for (i in 2:nrow(data)) {
    if((data$user_id[i] == data$user_id[i-1]) & (data$Name[i] == data$Name[i-1])) {
      data$DaysSince[i] <- difftime(strptime(data$Date[i], format = "%d.%m.%Y"),
                                           strptime(data$Date[i-1], format = "%d.%m.%Y"), units = "days")
    }
  }
  return(data)
}
```

##LengthTrip function

```{r}
## Computes the length of the trip for a visitor to a park
## Inputs: a twitter or flickr data frame
## Outputs: a twitter or flickr data frame
ComputeLenghtTrip <- function(data) {
  data$LengthTrip <- 0
  data$LengthTrip[data$DistanceDriven > 50 & data$DaysSince < 10 & !is.na(data$DaysSince)] <- NA

  for (i in 1:nrow(data)) {
   if (is.na(data$LengthTrip[i])) {
      next
   } else if (!is.na(data$LengthTrip[i + 1])) {
     data$LengthTrip[i] <- 1
    } else {
     j = i + 1
     while(j < nrow(data) & is.na(data$LengthTrip[j + 1])) {
       j = j +1
      }
      data$LengthTrip[i] <- sum(data$DaysSince[(i+1):j]) + 1
    }
  }
  return(data)
}
```


Maybe make a function that will actually get the Flickr data automatically
===

-This would have the same arguments at GetPhotosBBox, but will automatically loop through the years and get the data from it

-Troubles could be with exceeding the api limit and how to deal with this, might not be a problem though with Wisconsin as the most we got for 5 years was 1500 photos max (and rounding up) so thatd be roughly 1600 at max probably

-Could also not have to worry about this if were are just pulling data weekly/every 9 days alongside twitter

Other Flickr function ideas
===

-Make a get Unique data frame function easily?? just a thought.

## Twitter *next pulls on 7/30/18*

Lake Mendota

```{r}
mendota8.6.18 <- search_tweets(q = "Lake Mendota", n = 18000, include_rts = F, geocode = "43.104395,-89.415459,5mi")
ts_plot(mendota8.6.18)
mendota8.6.18$Name <- "Lake Mendota, WI"

##save(mendota7.16.18, file = "mendota7.16.18.rda")
##save(mendota7.23.18, file = "mendota7.23.18.rda")
##save(mendota7.30.18, file = "mendota7.30.18.rda")
##save(mendota8.6.18, file = "mendota8.6.18.rda")
```

Crater Lake

```{r}
crater8.6.18 <- search_tweets(q = "Crater Lake", n = 18000, include_rts = F, geocode = "42.945247,-122.108234,10mi")
ts_plot(crater8.6.18)
crater8.6.18$Name <- "Crater Lake, OR"

##save(crater7.16.18, file = "crater7.16.18.rda")
##save(crater7.23.18, file = "crater7.23.18.rda")
##save(crater7.30.18, file = "crater7.30.18.rda")
##save(crater8.6.18, file = "crater8.6.18.rda")
```

Devil's Lake State Park

```{r}
devils8.6.18 <- search_tweets(q = "Devil's Lake", n = 18000, include_rts = F, geocode = "43.415606,-89.730330,2mi")
ts_plot(devils8.6.18)
devils8.6.18$Name <- "Devil's Lake, WI"

##save(devils7.16.18, file = "devils7.16.18.rda")
##save(devils7.23.18, file = "devils7.23.18.rda")
##save(devils7.30.18, file = "devils7.30.18.rda")
##save(devils8.6.18, file = "devils8.6.18.rda")
```

Governer Dodge State Park

```{r}
dodge8.6.18 <- search_tweets(q = "Governer Dodge", n = 18000, include_rts = F, geocode = "43.025734,-90.103683,2.5mi")
ts_plot(dodge8.6.18)
dodge8.6.18 ##no tweets for this time period
```

Lake Kegonsa State Park

```{r}
kegonsa8.6.18 <- search_tweets(q = "Lake Kegonsa", n = 18000, include_rts = F, geocode = "42.968061,-89.244261,2mi")
ts_plot(kegonsa8.6.18)
kegonsa8.6.18$Name <- "Lake Kegonsa State Park, WI"

##save(kegonsa7.30.18, file = "kegonsa7.30.18.rda")
##save(kegonsa8.6.18, file = "kegonsa8.6.18.rda")
```

Yellowstone Lake State Park

```{r}
yellowstone8.6.18 <- search_tweets(q = "Yellowstone Lake", n = 18000, include_rts = F, geocode = "42.769699,-89.997597,2.5mi")
ts_plot(yellowstone8.6.18)
yellowstone8.6.18$Name <- "Yellowstone Lake State Park, WI"

##save(yellowstone8.6.18, file = "yellowstone8.6.18.rda")
```

Big Foot Beach State Park

```{r}
bigfoot8.6.18 <- search_tweets(q = "Big Foot", n = 18000, include_rts = F, geocode = "42.568885,-88.432732,1mi")
ts_plot(bigfoot8.6.18)
bigfoot8.6.18 ##no tweets for this time period
```

Mirror Lake State Park

```{r}
mirror8.6.18 <- search_tweets(q = "Mirror Lake", n = 18000, include_rts = F, geocode = "43.566732,-89.816150,1.5mi")
ts_plot(mirror8.6.18)
mirror8.6.18$Name <- "Mirror Lake State Park, WI"

##save(mirror7.16.18, file = "mirror7.16.18.rda")
##save(mirror7.23.18, file = "mirror7.23.18.rda")
##save(mirror7.30.18, file = "mirror7.30.18.rda")
##save(mirror8.6.18, file = "mirror8.6.18.rda")
```

Peninsula State Park

```{r}
peninsula8.6.18 <- search_tweets(q = "Peninsula", n = 18000, include_rts = F, geocode = "45.148148,-87.219086,2.5mi")
ts_plot(peninsula8.6.18)
peninsula8.6.18$Name <- "Peninsula State Park, WI"

##save(peninsula7.16.18, file = "peninsula7.16.18.rda")
##save(peninsula7.23.18, file = "peninsula7.23.18.rda")
##save(peninsula7.30.18, file = "peninsula7.30.18.rda")
##save(peninsula8.6.18, file = "peninsula8.6.18.rda")
```

Pattison State Park

```{r}
pattison8.6.18 <- search_tweets(q = "Pattison", n = 18000, include_rts = F, geocode = "46.529343,-92.120018,2mi")
ts_plot(pattison8.6.18)
pattison8.6.18 ##no tweets for this time period
```

Buckhorn State Park

```{r}
buckhorn8.6.18 <- search_tweets(q = "Buckhorn", n = 18000, include_rts = F, geocode = "43.934230,-89.995221,3mi")
ts_plot(buckhorn8.6.18)
buckhorn8.6.18$Name <- "Buckhorn State Park, WI"

##save(buckhorn7.16.18, file = "buckhorn7.16.18.rda")
##save(buckhorn7.23.18, file = "buckhorn7.23.18.rda")
##save(buckhorn7.30.18, file = "buckhorn7.30.18.rda")
##save(buckhorn8.6.18, file = "buckhorn8.6.18.rda")
```

Perrot State Park

```{r}
perrot8.6.18 <- search_tweets(q = "Perrot", n = 18000, include_rts = F, geocode = "44.024749,-91.490942,4.5mi")
ts_plot(perrot8.6.18)
perrot8.6.18 ##no tweets for this time period
```

Combine all of the twitter data sets that we have for the state parks
===

```{r}
twitter7.16.18 <- rbind(devils7.16.18, mirror7.16.18, peninsula7.16.18, buckhorn7.16.18)
twitter7.16.18$Year <- substr(twitter7.16.18$created_at, 1, 4)
twitter7.16.18$Month <- substr(twitter7.16.18$created_at, 6, 7)
twitter7.16.18$Day <- substr(twitter7.16.18$created_at, 9, 10)

##later, if we already have the feelings, might have to incorporate and average sentiment into this unique function
twitter7.16.18Unique <- twitter7.16.18%>%group_by(Name, location, user_id, Year, Month, Day)%>%summarise(Count = n())
twitter7.16.18Unique <- TwitterDistance(twitter7.16.18Unique)
twitter7.16.18Unique <- TwitterComputeDaysSince(twitter7.16.18Unique)
twitter7.16.18Unique <- ComputeLenghtTrip(twitter7.16.18Unique)
twitter7.16.18Unique

twitter7.16.18Unique%>%group_by(Name)%>%summarise(MeanDistance = mean(DistanceDriven, na.rm = T),
                                                  MedianDistance = median(DistanceDriven, na.rm = T),
                                                  MeanLengthTrip = mean(LengthTrip, na.rm = T),
                                                  MedianLengthTrip = median(LengthTrip, na.rm = T))
```

Combine to get sentiment analysis done
===

```{r}
twitter7.23.18 <- rbind(mendota7.16.18, mendota7.23.18, crater7.16.18, crater7.23.18, devils7.16.18, devils7.23.18, mirror7.16.18, mirror7.23.18, peninsula7.16.18, peninsula7.23.18, buckhorn7.16.18, buckhorn7.23.18)
dim(twitter7.23.18)
##save(twitter7.23.18, file = "twitter7.23.18.rda")

twitter7.30.18 <- rbind(mendota7.30.18, crater7.30.18, devils7.30.18, kegonsa7.30.18, mirror7.30.18, peninsula7.30.18, buckhorn7.30.18)
##save(twitter7.30.18, file = "twitter7.30.18.rda")

twitter8.6.18 <- rbind(mendota8.6.18, crater8.6.18, devils8.6.18, kegonsa8.6.18, yellowstone8.6.18, mirror8.6.18, peninsula8.6.18, buckhorn8.6.18)
##save(twitter8.6.18, file = "twitter8.6.18.rda") have not been sentiment analysis'ed
```


Look at the sentiments for these
===

```{r}
load("twitter7.23.18.sentiment.rda")
load("twitter7.23.18.no.ads.rda")
twitter7.23.18_sentiment_sen
twitter7.23.18_non_ad

twitterFiltered <- twitter7.23.18[-c(2,5,6,22,26,27,28,29,30,31,32,33,34,35,49,50,70,79,88,89,90,91,92,93,94,95,96,109,125,127,128,131,132,138,139,149,150),]

twitterSentiment <- twitter7.23.18_sentiment_sen%>%group_by(element_id)%>%summarise(Sentiment = sum(sentiment))
twitterSentiment

twitter <- cbind(twitterFiltered, twitterSentiment)
twitter2 <- TwitterDistance(twitter)
twitter2

ggplot(data = twitter30full, aes(x = as.factor(floor_date(created_at, unit = "1 week")), y = Sentiment)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), plot.title = element_text(size = 16)) +
  geom_boxplot(fill = "grey") +
  facet_wrap(~Name) +
  geom_hline(yintercept = 0, col = "red", lty = 2) +
  ggtitle("Tweets Sentiments per Week") +
  xlab("Week") +
  ylab("Sentiment Score")

##can zoom in just on Devil's Laeks graph, or do a percentage of tweets that are positive/negative/neutral for each week?
```

## The Next week's sentiment


```{r}
load("twitter7.30.18.sentiment.rda")
load("twitter7.30.18.no.ads.rda")
twitter7.30.18_sentiment_sen
twitter7.30.18_non_ad

twitter30filtered <- twitter7.30.18[-c(9,11,14,15,38,39,65,66,80),]

twitterSentiment2 <- twitter7.30.18_sentiment_sen%>%group_by(element_id)%>%summarise(Sentiment = sum(sentiment))
twitterSentiment2

twitter30 <- cbind(twitter30filtered, twitterSentiment2)
twitter30.a <- TwitterDistance(twitter30)

###
load("twitter8.6.18.sentiment.rda")
load("twitter8.6.18.no.ads.rda")
twitter8.6.18_sentiment_sen
twitter8.6.18_non_ad


twitter6filtered <- twitter8.6.18[-c(7,8,9,11,25,27,29,40,47,66,76,77),]

twitterSentiment3 <- twitter8.6.18_sentiment_sen%>%group_by(element_id)%>%summarise(Sentiment = sum(sentiment))

twitter6 <- cbind(twitter6filtered, twitterSentiment3)
twitter6.a <- TwitterDistance(twitter6)

##combining the twitter data sets and filter unique
twitter6full <- rbind(twitter2, twitter30.a, twitter6.a)
twitter6full <- unique(twitter30full)
```


## Distance

```{r}
ggplot(twitter6full%>%filter(!(Name %in% c("Lake Mendota, WI", "Crater Lake, OR"))), aes(x = DistanceDriven, y = Sentiment)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = rel(2)), axis.title.y = element_text(size = rel(2)), plot.title = element_text(size = rel(3))) +
  geom_point(size = 5) +
  geom_hline(yintercept = 0, col = "red", lty = 2, size = 2) +
  ylab(expression("Negative " %<-% " Sentiment " %->% " Positive")) +
  xlab("Distance Driven (miles)") +
  ggtitle("Sentiment vs Distance Driven to Wisconsin State Parks")
```

Zoom in on weeks
===

```{r}
devilsTwitter <- twitter6full %>%
                  filter(Name == "Devil's Lake, WI") %>%
                  mutate(Week = as.factor(floor_date(created_at, unit = "1 week")))
devilsTwitter$SentimentName <- ifelse(devilsTwitter$Sentiment > 0, "Positive",
                                      ifelse(devilsTwitter$Sentiment < 0, "Negative", "Neutral"))
devilsTwitterSimple <- devilsTwitter %>%
                  count(Week, SentimentName) %>%
                  rename(NumberTweets = n) %>%
                  mutate(TotalTweets = c(2,17,17,17,8,8,8,13,13,13,24,24,24,2,2), Percent = NumberTweets / TotalTweets)
                  
ggplot(data = devilsTwitterSimple, aes(x = Week, y = Percent, group = SentimentName, col = SentimentName)) +
  geom_point(size = 5, shape = 15)

##reformat to long version
devilsTwitterLong <- melt(devilsTwitterSimple) %>% filter(variable == "Percent")

devilsTwitterLong <- rbind(devilsTwitterLong, data.frame(Week = c("2018-07-01", "2018-07-01"), SentimentName = c("Negative", "Neutral"), variable = rep("Percent", 2), value = rep(0, 2)))

devilsTwitterLong$SentimentName <- as.factor(devilsTwitterLong$SentimentName)
devilsTwitterLong$SentimentName <- ordered(devilsTwitterLong$SentimentName, levels = c("Positive", "Neutral", "Negative"))


##plot
ggplot(data = devilsTwitterLong%>%filter(!(Week == "2018-07-01") & !(Week == "2018-08-05")), aes(x = Week, y = value, fill = SentimentName)) +
  geom_bar(stat = "identity", position = "dodge", col = "black") +
  theme_bw() +
  theme(axis.title.x = element_text(size = rel(2)), axis.title.y = element_text(size = rel(2)), plot.title = element_text(size = rel(3)), legend.key.size = unit(1.5, "cm"), legend.title = element_text(size = rel(2)), legend.text = element_text(size = rel(2)), axis.text.x = element_text(size = rel(2)), axis.text.y = element_text(size = rel(2))) +
  scale_fill_manual(values = c("blue3", "darkgrey", "red"), name = "Sentiment") +
  xlab("Week") +
  ylab("Percent of Tweets") +
  ggtitle("Devil's Lake Visitors' Sentiment")


##eventually want to get the weather along with that to see if that has any effect
```

Weather *not very important*
===

```{r}
weather <- read.csv("devilsWeather2.csv", header = T)
weather$Week <- c(rep("2018-07-01", 7), rep("2018-07-08", 7), rep("2018-07-15", 7))
weather <- weather %>%
            group_by(Week) %>%
            summarise(AvgTempMax = mean(TMAX), AvgTempMin = mean(TMIN), AvgPrcp = mean(PRCP))
weather
ggplot(data = weather, aes(x = Week)) +
  geom_point(aes(y = AvgTempMax), col = "red") +
  geom_point(aes(y = AvgTempMin), col = "blue") +
  geom_point(aes(y = AvgPrcp), col = "cyan")
```

